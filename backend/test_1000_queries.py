#!/usr/bin/env python3
"""
å¤§è§„æ¨¡è‡ªåŠ¨åŒ–è‡ªæˆ‘å­¦ä¹ æµ‹è¯•
æ‰§è¡Œ1000æ¬¡è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œæ£€æµ‹ç»“æœï¼Œå®ç°è‡ªåŠ¨å­¦ä¹ å¾ªç¯
"""
import sys
sys.path.insert(0, '/opt/sqlbot/app')

import random
import time
from datetime import datetime
from typing import Dict, List, Tuple, Any

from apps.datasource.embedding.db_description_parser import DatabaseDescriptionParser
from apps.datasource.embedding.semantic_search import get_semantic_search_engine
from apps.datasource.embedding.db_context_injector import DatabaseContextInjector
from apps.datasource.embedding.self_learning import (
    SelfLearningEngine,
    get_self_learning_engine,
    record_user_feedback
)


class LargeScaleLearningTester:
    """å¤§è§„æ¨¡å­¦ä¹ æµ‹è¯•å™¨"""

    def __init__(self):
        self.parser = DatabaseDescriptionParser('/opt/sqlbot/app/æ•°æ®åº“æè¿°.md')
        self.modules = self.parser.parse()
        self.semantic_engine = get_semantic_search_engine()
        self.injector = DatabaseContextInjector()
        self.learning_engine = get_self_learning_engine()

        self.test_queries = self._generate_1000_queries()

    def _generate_1000_queries(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆ1000ä¸ªæµ‹è¯•æŸ¥è¯¢"""
        queries = []

        noun_keywords = {
            "èµ„äº§": ["assets"],
            "èµ„äº§åˆ—è¡¨": ["assets"],
            "èµ„äº§ä¿¡æ¯": ["assets"],
            "èµ„äº§æƒ…å†µ": ["assets"],
            "èµ„äº§çŠ¶æ€": ["assets"],
            "èµ„äº§ç»Ÿè®¡": ["assets"],
            "èµ„äº§åˆ†ç±»": ["asset_categories"],
            "èµ„äº§ä½ç½®": ["asset_locations"],

            "ç›˜ç‚¹": ["inventory_records"],
            "ç›˜ç‚¹è®°å½•": ["inventory_records"],
            "ç›˜ç‚¹åˆ—è¡¨": ["inventory_records"],
            "ç›˜ç‚¹ç»“æœ": ["inventory_details"],
            "ç›˜ç‚¹æ˜ç»†": ["inventory_details"],

            "ç»´ä¿®": ["maintenance_workorders"],
            "ç»´ä¿®å·¥å•": ["maintenance_workorders"],
            "ç»´æŠ¤è®°å½•": ["maintenance_records"],
            "ä¿å…»è®¡åˆ’": ["maintenance_plans"],
            "æ•…éšœè®°å½•": ["maintenance_records"],

            "éªŒæ”¶": ["acceptance_applications"],
            "éªŒæ”¶ç”³è¯·": ["acceptance_applications"],
            "éªŒæ”¶è®°å½•": ["acceptance_applications"],
            "ç­¾å­—": ["acceptance_application_signatures"],
            "æ–‡ä»¶": ["acceptance_application_files"],

            "ä¸è‰¯äº‹ä»¶": ["adverse_reaction_records"],
            "æ•…éšœæŠ¥å‘Š": ["adverse_reaction_records"],
            "äº‹æ•…": ["adverse_reaction_records"],
            "äº‹ä»¶ç­‰çº§": ["adverse_reaction_records"],
            "ä¸¥é‡ç¨‹åº¦": ["adverse_reaction_records"],

            "è´¨æ§": ["quality_control_records"],
            "è´¨æ§è®°å½•": ["quality_control_records"],
            "è´¨é‡æ§åˆ¶": ["quality_control_records"],
            "è´¨æ§ç»“æœ": ["quality_control_records"],
            "é¢„è­¦": ["quality_management_alerts"],

            "è®¡é‡": ["metrology_records"],
            "è®¡é‡è®°å½•": ["metrology_records"],
            "æ£€æµ‹ç»“æœ": ["metrology_records"],
            "æ£€æµ‹è®¡åˆ’": ["metrology_plans"],

            "è°ƒé…": ["transfer_records"],
            "è°ƒé…è®°å½•": ["transfer_records"],
            "èµ„äº§è°ƒé…": ["transfer_records"],
            "è°ƒæ‹¨": ["transfer_applications"],

            "æŠ¥åºŸ": ["scrapped_records"],
            "æŠ¥åºŸè®°å½•": ["scrapped_records"],
            "æŠ¥åºŸç”³è¯·": ["scrapped_applications"],

            "æ—¥å¿—": ["operation_logs"],
            "ç”¨æˆ·": ["users"],
            "è§’è‰²": ["roles"],
            "ä»»åŠ¡": ["task_records"],
            "å‘Šè­¦": ["alert_records"],
            "AIé—®ç­”": ["ai_conversations"],
            "ä¼šè¯": ["ai_conversations"],
        }

        actions = ["æŸ¥è¯¢", "æŸ¥çœ‹", "è·å–", "ç»Ÿè®¡", "åˆ—å‡º", "å±•ç¤º"]
        modifiers = ["", "åˆ—è¡¨", "ä¿¡æ¯", "æƒ…å†µ", "çŠ¶æ€", "ç»Ÿè®¡", "æŸ¥è¯¢", "æŸ¥çœ‹", "è®°å½•", "è¯¦æƒ…", "å†å²", "æœ€æ–°", "ä»Šå¤©", "æœ¬å‘¨", "æœ¬æœˆ", "æ‰€æœ‰", "å…¨éƒ¨", "å¤šå°‘", "å“ªäº›"]

        for i in range(1000):
            if random.random() < 0.3:
                noun, tables = random.choice(list(noun_keywords.items()))
                action = random.choice(actions)
                question = f"{action}{noun}"
            elif random.random() < 0.3:
                noun, tables = random.choice(list(noun_keywords.items()))
                modifier = random.choice(modifiers)
                question = f"{modifier}{noun}" if modifier else noun
            else:
                noun, tables = random.choice(list(noun_keywords.items()))
                question = noun

            query = {
                'id': i + 1,
                'question': question,
                'tables': tables,
                'keywords': [noun],
                'sql': f"SELECT * FROM {tables[0]} LIMIT 1000"
            }
            queries.append(query)

        return queries

    def run_test(self) -> Dict[str, Any]:
        """è¿è¡Œå¤§è§„æ¨¡æµ‹è¯•"""
        print('='*80)
        print('å¤§è§„æ¨¡è‡ªåŠ¨åŒ–è‡ªæˆ‘å­¦ä¹ æµ‹è¯• - 1000æ¬¡æŸ¥è¯¢')
        print('='*80)
        print(f'\næµ‹è¯•æ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
        print(f'æµ‹è¯•æŸ¥è¯¢æ•°: {len(self.test_queries)}')

        results = {
            'total': len(self.test_queries),
            'success': 0,
            'failed': 0,
            'feedback_positive': 0,
            'feedback_negative': 0,
            'query_results': []
        }

        print('\n[1/3] æ„å»ºè¯­ä¹‰å‘é‡ç´¢å¼•...')
        if not self.semantic_engine.index_built:
            self.semantic_engine.build_index(self.modules, force=True)
        print(f'      ç´¢å¼•è¡¨æ•°é‡: {len(self.semantic_engine.table_vectors)}')

        print('\n[2/3] æ‰§è¡Œ1000æ¬¡æŸ¥è¯¢å¹¶è®°å½•åé¦ˆ...')
        print('-'*80)

        start_time = time.time()

        for i, query in enumerate(self.test_queries, 1):
            if i % 100 == 0:
                elapsed = time.time() - start_time
                eta = (elapsed / i) * (len(self.test_queries) - i)
                print(f'\r      è¿›åº¦: {i}/{len(self.test_queries)} ({i*100//len(self.test_queries)}%) '
                      f'å·²ç”¨æ—¶: {elapsed:.1f}ç§’ é¢„è®¡å‰©ä½™: {eta:.1f}ç§’', end='', flush=True)

            question = query['question']
            expected_tables = query['tables']

            semantic_results = self.semantic_engine.search(question, top_k=3)
            matched_tables = [r.table_name for r in semantic_results[:3]]

            is_success = len(set(matched_tables) & set(expected_tables)) > 0

            if is_success:
                results['success'] += 1
                feedback = 'positive'
                results['feedback_positive'] += 1
            else:
                results['failed'] += 1
                feedback = 'negative'
                results['feedback_negative'] += 1

            record_user_feedback(
                question=question,
                generated_sql=query['sql'],
                feedback=feedback,
                matched_tables=matched_tables,
                matched_fields=[],
                matched_enums=[]
            )

            results['query_results'].append({
                'id': query['id'],
                'question': question,
                'expected': expected_tables,
                'matched': matched_tables,
                'success': is_success
            })

        total_time = time.time() - start_time
        print('\n')

        print('\n[3/3] åˆ†æå­¦ä¹ æ•ˆæœ...')
        print('-'*80)

        stats = self.learning_engine.get_learning_stats()

        print('\n' + '='*80)
        print('å¤§è§„æ¨¡æµ‹è¯•ç»“æœç»Ÿè®¡')
        print('='*80)

        success_rate = results['success'] / results['total'] * 100

        print(f'\næŸ¥è¯¢ç»Ÿè®¡:')
        print(f'   æ€»æŸ¥è¯¢æ•°: {results["total"]}')
        print(f'   æˆåŠŸåŒ¹é…: {results["success"]} ({success_rate:.1f}%)')
        print(f'   å¤±è´¥åŒ¹é…: {results["failed"]} ({100-success_rate:.1f}%)')
        print(f'   æ€»è€—æ—¶: {total_time:.1f}ç§’')
        print(f'   å¹³å‡æ¯æŸ¥è¯¢: {total_time/results["total"]*1000:.1f}æ¯«ç§’')

        print(f'\nåé¦ˆç»Ÿè®¡:')
        print(f'   æ­£é¢åé¦ˆ: {results["feedback_positive"]}')
        print(f'   è´Ÿé¢åé¦ˆ: {results["feedback_negative"]}')

        print(f'\nå­¦ä¹ ç»Ÿè®¡:')
        print(f'   å­¦ä¹ æ¨¡å¼æ•°: {stats["learned_patterns"]}')
        print(f'   å…³é”®è¯æƒé‡æ•°: {stats["keyword_weights"]}')
        print(f'   è®°å¿†æ¡ç›®æ•°: {stats["memory_items"]}')

        print(f'\né«˜é¢‘å…³é”®è¯ (Top 20):')
        for kw in stats["top_keywords"][:20]:
            print(f'   - {kw["keyword"][:30]}: æƒé‡={kw["weight"]:.2f}, æˆåŠŸ={kw["success"]}')

        print(f'\næˆåŠŸæ¨¡å¼ (Top 10):')
        for pattern in stats["top_patterns"][:10]:
            print(f'   - {pattern["pattern"][:40]}... æˆåŠŸ={pattern["success"]}, ç½®ä¿¡åº¦={pattern["confidence"]:.2f}')

        print(f'\nå¤±è´¥æŸ¥è¯¢ç¤ºä¾‹ (å‰20ä¸ª):')
        failed_queries = [r for r in results['query_results'] if not r['success']]
        for r in failed_queries[:20]:
            print(f'   âŒ "{r["question"][:30]}" (æœŸæœ›: {r["expected"]}, åŒ¹é…: {r["matched"][:2]})')

        print('\n' + '='*80)

        return results


def main():
    """ä¸»å‡½æ•°"""
    tester = LargeScaleLearningTester()
    results = tester.run_test()

    print('\nâœ… å¤§è§„æ¨¡è‡ªåŠ¨åŒ–è‡ªæˆ‘å­¦ä¹ æµ‹è¯•å®Œæˆ!')
    print(f'\nğŸ“Š æ€»ç»“:')
    print(f'   - æ‰§è¡Œäº† {results["total"]} æ¬¡æŸ¥è¯¢')
    print(f'   - æˆåŠŸç‡: {results["success"]/results["total"]*100:.1f}%')
    print(f'   - æ”¶é›†äº† {results["feedback_positive"]} æ¡æ­£é¢åé¦ˆ')
    print(f'   - æ”¶é›†äº† {results["feedback_negative"]} æ¡è´Ÿé¢åé¦ˆ')
    print(f'   - ç³»ç»Ÿå·²è‡ªåŠ¨å­¦ä¹ è¿™äº›åé¦ˆå¹¶æŒç»­ä¼˜åŒ–è¯­ä¹‰ç†è§£')


if __name__ == '__main__':
    main()
